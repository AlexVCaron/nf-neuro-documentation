---
title: Steps 5 to 8
description: Learn how to build a simple pipeline using nf-neuro components
---

import CommandOutputs from '../../../components/CommandOutputs.astro';
import { Steps } from '@astrojs/starlight/components';
import { Tabs, TabItem } from '@astrojs/starlight/components';
import { FileTree } from '@astrojs/starlight/components';
import { Code } from '@astrojs/starlight/components';
import Runtime from "@mdx-js/runtime";

### Step 5 :  Install and Use an nf-neuro Subworkflow

#### Listing and Installing an nf-neuro Subworkflow

Similar to modules, you can list available `subworkflows` in a remote repository or those 
installed locally in your pipeline using the `nf-core subworkflows list` command. 
To view all the available subworkflows, use the `nf-core subworkflows list remote` command, 
which will display all subworkflows in the terminal.

   <CommandOutputs>
   <span slot="command">
    ```bash
       nf-core subworkflows list remote
    ```
   </span>
   <span slot="output">
   You should see this output:
    ```bash

                                            ,--./,-.
              ___     __   __   __   ___   /,-._.--~\
        |\ | |__  __ /  ` /  \ |__) |__       }  {
        | \| |       \__, \__/ |  \ |___   \`-._,-`-,
                                            `._,._,'

        nf-core/tools version 2.14.1 - https://nf-co.re
        There is a new version of nf-core/tools available! (3.2.0)


    INFO     Subworkflows available from https://github.com/scilus/nf-neuro.git (main):                                                                                                          
                                                                                                                                                                                                
    ┏━━━━━━━━━━━━━━━━━━━━━━━━━┓
    ┃ Subworkflow Name        ┃
    ┡━━━━━━━━━━━━━━━━━━━━━━━━━┩
    │ anatomical_segmentation │
    │ bundle_seg              │
    │ io_bids                 │
    │ load_test_data          │
    │ preproc_dwi             │
    │ preproc_t1              │
    │ registration            │
    │ topup_eddy              │
    │ tractoflow              │
    └─────────────────────────┘
    ```

   </span>
   </CommandOutputs>


Now, let's install the `PREPROC_T1` subworkflow, designed to preprocess T1-weighted MRI data.  
You can install the subworkflow using the `nf-core subworkflows install` command. 
The subworkflow will be installed in the `./subworkflows/nf-neuro/` directory.

   <CommandOutputs>
   <span slot="command">
    ```bash
       nf-core subworkflows install preproc_T1
    ```
   </span>
   <span slot="output">
   You should see this output:
    ```bash

                                            ,--./,-.
            ___     __   __   __   ___     /,-._.--~\
        |\ | |__  __ /  ` /  \ |__) |__         }  {
        | \| |       \__, \__/ |  \ |___     \`-._,-`-,
                                            `._,._,'

        nf-core/tools version 2.14.1 - https://nf-co.re
        There is a new version of nf-core/tools available! (3.2.0)


    INFO     Installing 'preproc_t1'                                                                                                                                                             
    INFO     Use the following statement to include this subworkflow:                                                                                                                            
                                                                                                                                                                                                
    include { PREPROC_T1 } from '../subworkflows/nf-neuro/preproc_t1/main' 
    ```

   </span>
   </CommandOutputs>


#### Add New Data in the Input Pipeline Structure

To include a T1w image in the pipeline, follow the approach from **Step 2** for adding DWI images.
The necessary steps are: 

    <Tabs>
    <TabItem label="Tasks">
    <Steps>

    1. Add T1w image path to `get_data` input structure.

    2. `emit` a new output channel called `anat`.

    3. Include the T1 channel data in the worflow and visualize it.

    4. Run the pipeline using `nextflow run main.nf --input data -profile docker`.

    </Steps>

    </TabItem>
    <TabItem label="Expected main.nf">
    ```nextflow
        #!/usr/bin/env nextflow

        include { RECONST_DTIMETRICS } from './modules/nf-neuro/reconst/dtimetrics/main'
        include { DENOISING_MPPCA } from './modules/nf-neuro/denoising/mppca/main'
        include { PREPROC_T1 } from './subworkflows/nf-neuro/preproc_t1/main'

        workflow get_data {
            main:
                if ( !params.input ) {
                    log.info "You must provide an input directory containing all images using:"
                    log.info ""
                    log.info "    --input=/path/to/[input]   Input directory containing your subjects"
                    log.info "                        |"
                    log.info "                        ├-- S1"
                    log.info "                        |    ├-- *dwi.nii.gz"
                    log.info "                        |    ├-- *dwi.bval"
                    log.info "                        |    ├-- *dwi.bvec"
                    log.info "                        |    └-- *t1.nii.gz"
                    log.info "                        └-- S2"
                    log.info "                             ├-- *dwi.nii.gz"
                    log.info "                             ├-- *bval"
                    log.info "                             ├-- *bvec"
                    log.info "                             └-- *t1.nii.gz"
                    log.info ""
                    error "Please resubmit your command with the previous file structure."
                }

                input = file(params.input)
                // ** Loading DWI files. ** //
                dwi_channel = Channel.fromFilePairs("$input/**/**/dwi/*dwi.{nii.gz,bval,bvec}", size: 3, flat: true)
                    { it.parent.parent.parent.name + "_" + it.parent.parent.name} // Set the subject filename as subjectID + '_' + session.
                    .map{ sid, bvals, bvecs, dwi -> [ [id: sid], dwi, bvals, bvecs ] } // Reordering the inputs.
                // ** Loading T1 file. ** //
                t1_channel = Channel.fromFilePairs("$input/**/**/anat/*T1w.nii.gz", size: 1, flat: true)
                    { it.parent.parent.parent.name + "_" + it.parent.parent.name } // Set the subject filename as subjectID + '_' + session.
                    .map{ sid, t1 -> [ [id: sid], t1 ] }
            emit:
                dwi = dwi_channel 
                anat = t1_channel
        }

        workflow {
            inputs = get_data()

            // Use Multimap to split the tuple into multi inputs structure
            ch_dwi_bvalbvec = inputs.dwi
                .multiMap { meta, dwi, bval, bvec ->
                    dwi:            [ meta, dwi ]
                    bvs_files:      [ meta, bval, bvec ]
                    dwi_bval_bvec:  [ meta, dwi, bval, bvec ]
                }

            // Denoising DWI
            input_dwi_denoise = ch_dwi_bvalbvec.dwi
                            .map{ it + [[]] }
            DENOISING_MPPCA( input_dwi_denoise )

            // Fetch specific output
            ch_dwi_denoised = DENOISING_MPPCA.out.image

            // Input DTI update with DWI denoised output
            input_dti_denoised = ch_dwi_denoised
                    .join(ch_dwi_bvalbvec.bvs_files)
                    .map{ it + [[]] }

            // DTI-derived metrics
            RECONST_DTIMETRICS( input_dti_denoised )

            // Preprocessing T1 images
            inputs.anat.view()

        }
    ```
    </TabItem>
    <TabItem label="Expected output">
    ```bash
    Launching `main.nf` [evil_noether] DSL2 - revision: f131ccc34c

    executor >  local (2)
    [c8/fa8ee7] process > DENOISING_MPPCA (sub-003_ses-01)    [100%] 1 of 1 ✔
    [e9/6b32bf] process > RECONST_DTIMETRICS (sub-003_ses-01) [100%] 1 of 1 ✔

    [[id:sub-003_ses-01], /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/anat/sub-003_ses-01_T1w.nii.gz]
    ```
    </TabItem>
    </Tabs>


### Bind the Subworkflow and Prepare the Input subworkflow Structure

 <Steps>

 1. Bind the subworkflow as follows:

    ```nextflow
    PREPROC_T1( input_channel(s) )
    ```

 2. Prepare structure input for subworkflow. 

    As with modules, the API documentation also lists **subworkflows**, 
    providing information on **Inputs**, **Outputs**, and the module list used in the 
    **Components** section:  [PREPROC_T1](https://scilus.github.io/nf-neuro/api/subworkflows/preproc_t1/).

    The PREPROC_T1 process has 7 input channels defined, with only ch_image being mandatory.
    Just like using an empty list for modules, using an **empty channel** allows the 
    process to run without optional inputs.

        - ch_image (Mandatory)
        - ch_template (Optional)
        - ch_probability_map (Optional)
        - ch_mask_nlmeans (Optional)
        - ch_ref_n4 (Optional)
        - ch_ref_resample (Optional)
        - ch_weights (Optional)

    <Tabs>
    <TabItem label="Tasks">

    Use the `Channel.empty()` function for optional inputs, try to define the input structure for PREPROC_T1.

    </TabItem>
    <TabItem label="Expected command">
    ```nextflow

        PREPROC_T1(
            inputs.anat,
            Channel.empty(),
            Channel.empty(),
            Channel.empty(),
            Channel.empty(),
            Channel.empty(),
            Channel.empty()
        )
    ```
    </TabItem>
    </Tabs>

 </Steps>


### Configure your subworkflow

Configuring a subworkflow requires understanding the modules used and the associated parameters.

The [PREPROC_T1](https://scilus.github.io/nf-neuro/api/subworkflows/preproc_t1/) API documentation 
lists the modules included in the subworkflow in the **Components** section : 

        - denoising/nlmeans
        - preproc/n4
        - image/resample
        - betcrop/antsbet
        - betcrop/synthbet
        - image/cropvolume

However, it does not yet provide a list of the parameters associated with the subworkflows. 
You can find the parameters for the subworkflow in the `./subworkflow/nf-neuro/preproc_t1/main.nf` 
file or the `nextflow.config` file within the **tests** folder (`./subworkflow/nf-neuro/preproc_t1/tests/nextflow.config`).

For simplicity's sake, we will enable the `denoising` and `synthbet` options (set to `true`) and 
disable all other options (set to `false`). 

Add the following to your `nextflow.config` file:

```nextflow
    // Add those line after params.output = 'result'

    // ** Subworkflow PREPROC T1 **
    params.preproc_t1_run_denoising = true
    params.preproc_t1_run_N4 = true
    params.preproc_t1_run_resampling = false
    params.preproc_t1_run_ants_bet = false
    params.preproc_t1_run_synthbet = true
    params.preproc_t1_run_crop = false

```

Since we are disabling all options except for `denoising` and `synthbet`, we only need to 
include the parameters for the modules corresponding to those options. 
Note that the `denoising` module does not require any specific parameters, so it can be ignored in the configuration.

Now, update your `nextflow.config` file by adding the specific options for the subworkflow in the process section as follows:

```nextflow
    // Add those line in the process {} after DENOISING_MPPCA :

    withName: "BETCROP_SYNTHBET" {
        memory = "4G"
        ext.nocsf = false
    }
```

Now you can run and validate your pipeline:

<Tabs>
<TabItem label="Check if it works">
If you don't want to recompute each image, you can add the `-resume` option to your command line.
```bash
nextflow run main.nf --input data -profile docker 
```
</TabItem>
<TabItem label="Expected output">
```bash
    N E X T F L O W   ~  version 24.10.4

    Launching `main.nf` [dreamy_lichterman] DSL2 - revision: 1af7463284

    executor >  local (4)
    [4a/908262] DENOISING_MPPCA (sub-003_ses-01)              [100%] 1 of 1 ✔
    [29/a93bfe] RECONST_DTIMETRICS (sub-003_ses-01)           [100%] 1 of 1 ✔
    [f5/79c402] PREPROC_T1:DENOISING_NLMEANS (sub-003_ses-01) [100%] 1 of 1 ✔
    [ef/10c1d5] PREPROC_T1:BETCROP_SYNTHBET (sub-003_ses-01)  [100%] 1 of 1 ✔
    Completed at: Date Hour
    Duration    : 6m 14s
    CPU hours   : 0.1
    Succeeded   : 4
```

</TabItem>
<TabItem label="Expected main.nf">
```nextflow
    #!/usr/bin/env nextflow

    include { RECONST_DTIMETRICS } from './modules/nf-neuro/reconst/dtimetrics/main'
    include { DENOISING_MPPCA } from './modules/nf-neuro/denoising/mppca/main'
    include { PREPROC_T1 } from './subworkflows/nf-neuro/preproc_t1/main'

    workflow get_data {
        main:
            if ( !params.input ) {
                log.info "You must provide an input directory containing all images using:"
                log.info ""
                log.info "    --input=/path/to/[input]   Input directory containing your subjects"
                log.info "                        |"
                log.info "                        ├-- S1"
                log.info "                        |    ├-- *dwi.nii.gz"
                log.info "                        |    ├-- *dwi.bval"
                log.info "                        |    ├-- *dwi.bvec"
                log.info "                        |    └-- *t1.nii.gz"
                log.info "                        └-- S2"
                log.info "                             ├-- *dwi.nii.gz"
                log.info "                             ├-- *bval"
                log.info "                             ├-- *bvec"
                log.info "                             └-- *t1.nii.gz"
                log.info ""
                error "Please resubmit your command with the previous file structure."
            }

            input = file(params.input)
            // ** Loading DWI files. ** //
            dwi_channel = Channel.fromFilePairs("$input/**/**/dwi/*dwi.{nii.gz,bval,bvec}", size: 3, flat: true)
                { it.parent.parent.parent.name + "_" + it.parent.parent.name} // Set the subject filename as subjectID + '_' + session.
                .map{ sid, bvals, bvecs, dwi -> [ [id: sid], dwi, bvals, bvecs ] } // Reordering the inputs.
            // ** Loading T1 file. ** //
            t1_channel = Channel.fromFilePairs("$input/**/**/anat/*T1w.nii.gz", size: 1, flat: true)
                { it.parent.parent.parent.name + "_" + it.parent.parent.name } // Set the subject filename as subjectID + '_' + session.
                .map{ sid, t1 -> [ [id: sid], t1 ] }
        emit:
            dwi = dwi_channel 
            anat = t1_channel
    }

    workflow {
        inputs = get_data()

        // Use Multimap to split the tuple into multi inputs structure
        ch_dwi_bvalbvec = inputs.dwi
            .multiMap { meta, dwi, bval, bvec ->
                dwi:            [ meta, dwi ]
                bvs_files:      [ meta, bval, bvec ]
                dwi_bval_bvec:  [ meta, dwi, bval, bvec ]
            }

        // Denoising DWI
        input_dwi_denoise = ch_dwi_bvalbvec.dwi
                        .map{ it + [[]] }
        DENOISING_MPPCA( input_dwi_denoise )

        // Fetch specific output
        ch_dwi_denoised = DENOISING_MPPCA.out.image

        // Input DTI update with DWI denoised output
        input_dti_denoised = ch_dwi_denoised
                .join(ch_dwi_bvalbvec.bvs_files)
                .map{ it + [[]] }

        // DTI-derived metrics
        RECONST_DTIMETRICS( input_dti_denoised )

        // Preprocessing T1 images
        //inputs.anat.view()
        
        PREPROC_T1(
            input_t1,
            Channel.empty(),
            Channel.empty(),
            Channel.empty(),
            Channel.empty(),
            Channel.empty(),
            Channel.empty()
        )

    }
```
</TabItem>
<TabItem label="Expected nextflow.config">
```nextflow
    profiles {
        docker {
            docker.enabled          = true
            conda.enabled           = false
            singularity.enabled     = false
            podman.enabled          = false
            shifter.enabled         = false
            charliecloud.enabled    = false
            apptainer.enabled       = false
            docker.runOptions       = '-u $(id -u):$(id -g)'
        }
    }

    manifest {
        name            = 'scilus/nf-neuro-tutorial'
        description     = """nf-neuro-tutorial is a Nextflow pipeline for processing neuroimaging data."""
        version         = '0.1dev'
    }

    params.input      = false
    params.output     = 'result'

    // ** Subworkflow PREPROC T1 **
    params.preproc_t1_run_denoising = true
    params.preproc_t1_run_N4 = false
    params.preproc_t1_run_resampling = false
    params.preproc_t1_run_ants_bet = false
    params.preproc_t1_run_synthbet = true
    params.preproc_t1_run_crop = false


    process {

        publishDir = { "${params.output}/$meta.id/${task.process.replaceAll(':', '-')}" }

        withName: "DENOISING_MPPCA" {
            ext.extent = 3
        }

        withName: "BETCROP_SYNTHBET" {
            memory = "4G"
            ext.nocsf = false
        }

        withName: "RECONST_DTIMETRICS" {
            ext.ad = false
            ext.evecs = false
            ext.evals = false
            ext.fa = true
            ext.ga = false
            ext.rgb = false
            ext.md = true
            ext.mode = false
            ext.norm = false
            ext.rd = false
            ext.tensor = false
            ext.nonphysical = false
            ext.pulsation = false
            ext.residual = false
            ext.b0_thr_extract_b0 = 10
            ext.dwi_shell_tolerance = 50
            ext.max_dti_shell_value = 1200
            ext.run_qc = false
        }
    }
    ```
</TabItem>
</Tabs>



### Step 6 : Create your own local module 

In this step, we will create a local module called `METRICSINROI` under the `STATS` category, 
designed for segmenting a T1-weighted (T1w) image and extracting metrics from white matter (WM), 
gray matter (GM), and cerebrospinal fluid (CSF) masks.

#### Creating a Local Module structure

    Create the following directory structure, including a main.nf file inside the module folder:

    `stats` = category and `metricsinrois` = module name
    ```
    nf-neuro-tutorial/
    ├── ...
    ├── modules/
    │   ├── local/
    │   │   ├── stats/metricsinrois/main.nf
    │   ├── nf-neuro/
    |   |   |--categories/modules
    └── ...
    ```

### Write the local module

(faire le lien avec contribute ?, je dirais non, a voir)


A Nextflow module is a file containing one process that execute one or more command. 
The structure of a module should look like this:

    <Tabs>
    <TabItem label="Example of module">

    ```
    process CATEGORY_MODULE {
        tag "$meta.id"
        label 'process_single'

        container "${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?
            'https://scil.usherbrooke.ca/containers/scilus_latest.sif':
            'scilus/scilus:latest' }"

        input:
        tuple val(meta), path(input_file)
        
        output:
        tuple val(meta), path("output.txt") , emit: outputname
        
        script:
        def prefix = task.ext.prefix ?: "${meta.id}"
        def optionName = task.ext.option ? "${task.ext.option}" : "default_value"
        """
        cat ${input_file} > output.txt
        """
    }
    ```

    </TabItem>
    <TabItem label="Let's break it down">
    <Steps>
    1. `tag "$meta.id"`
        This assigns a tag to the process using the 'id' field from the 'meta' map. 
        Tags are useful for logging and tracking purposes.

    2. `label 'process_single'`
        This assigns a label to the process, which can be used for 
        resource allocation or other configuration purposes.

    3. `container`
        Specifies the container to use for the process. 
        It conditionally chooses between a Singularity or Docker container based on the workflow configuration.

    4. `Input`
        Defines the input for the process as a tuple containing 
        metadata (meta) and an input file path.

    5. `Output`
        Defines the output of the process as a tuple containing 
        the input metadata and a file named "output.txt". 
        The output is emitted to a channel named "outputname".

    6. `script`
        This is the script that will be executed by the process. 
        It defines some option a `prefix` variable using a `task` extension or the meta id.
        Defines an 'optionName' variable using a task extension or a default value.

        Here it, execute a simple command to copy the contents of the input file to output.txt.
    </Steps>

    In summary, this process takes an input file, copies its contents to an output file,
     and provides some flexibility in naming and options through the use of metadata and task extensions. 
     The process can run in either a Singularity or Docker container, depending on the workflow configuration.

    </TabItem>
    <TabItem label="Tasks">

    The goal of this local module is to segment a T1-weighted (T1w) image using `fast` and 
    extract metrics from the white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) 
    masks using the `scil_volume_stats_in_ROI.py` script.

    You can copy and paste the example module into your `main.nf` file. First, rename the process 
    `STATS_METRICSINROI` and then modify it using the steps below:

    <Steps>
    
    1. Modify `input` to include T1 and metrics
        ```nextflow
        tuple val(meta), path(t1), path(metrics)
        ```

    2. Update `output` to include segmentation maps and masks, as well as the JSON output from silpy script. 
        Example for WM data :

        ```nextflow
            output:
            tuple val(meta), path("*.json")                         , emit: stats
            tuple val(meta), path("*mask_wm.nii.gz")                , emit: wm_mask
            tuple val(meta), path("*map_wm.nii.gz")                 , emit: wm_map

        ```

    3. Include `task` extensions for additional options (those from scilpy script):

        ```nextflow
            script:
            def prefix = task.ext.prefix ?: "${meta.id}"
            def bin = task.ext.bin ? "--bin " : ""
            def normalize_weights = task.ext.normalize_weights ? "--normalize_weights " : ""
        ```

    4. Modify the `script`
        <Steps>
            1. Update the script to perform segmentation using `fast` on the T1w image.

            2. Generate binary masks for each PVE tissue segmentation using `scil_volume_math.py` or another tool.
            
            3. Rename PVE outputs from `fast` to map_*.nii.gz and mask_*.nii.gz

            4. Extract metrics from the WM, GM and CSF binary masks using `scil_volume_stats_in_ROI.py` or other.
        </Steps>

        :::note
            ### **Add a `meta.yml` File for Local Modules and Subworkflows**  

            When working with local modules in Nextflow, you can add a `meta.yml` file to store metadata. 
            This file provides useful details about the module or subworkflow, making it easier to understand and use.  

            #### **Why Use `meta.yml`?**  

            - It serves as a metadata file that contains essential information about a module or subworkflow, 
            and helps document the component, making it more accessible to other users and developers.  

            #### **Where to Place the `meta.yml` File?** 

            - **For Modules:** `./modules/nf-core/<module_name>/meta.yml`  
            - **For Subworkflows:** `./subworkflows/nf-core/<subworkflow_name>/meta.yml`  

            #### **What Does `meta.yml` Contain?**  

            - General information about the module or subworkflow  
            - Author details  
            - A brief description of input and output files used in the main script (specific to each module or subworkflow)  

            If you're looking for an **example**, you can find a `meta.yml` file in the nf-neuro repository, within its  
            modules/subworkflows directory. This example provides a structured reference for how to document your own modules/subworkflows.

            By including a `meta.yml` file, you ensure better documentation and maintainability of your Nextflow components.
        :::

    </Steps>

    </TabItem>
    <TabItem label="Expected module">
    ```nextflow
        process STATS_METRICSINROI {
            tag "$meta.id"
            label 'process_single'

            container "${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?
                'https://scil.usherbrooke.ca/containers/scilus_latest.sif':
                'scilus/scilus:latest' }"

            input:
            tuple val(meta), path(t1), path(metrics)

            output:
            tuple val(meta), path("*.json")                         , emit: stats
            tuple val(meta), path("*mask_wm.nii.gz")                , emit: wm_mask
            tuple val(meta), path("*mask_gm.nii.gz")                , emit: gm_mask
            tuple val(meta), path("*mask_csf.nii.gz")               , emit: csf_mask
            tuple val(meta), path("*map_wm.nii.gz")                 , emit: wm_map
            tuple val(meta), path("*map_gm.nii.gz")                 , emit: gm_map
            tuple val(meta), path("*map_csf.nii.gz")                , emit: csf_map

            script:
            def prefix = task.ext.prefix ?: "${meta.id}"
            def bin = task.ext.bin ? "--bin " : ""
            def normalize_weights = task.ext.normalize_weights ? "--normalize_weights " : ""
            """
            export ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS=1
            export OMP_NUM_THREADS=1
            export OPENBLAS_NUM_THREADS=1

            fast -t 1 -n 3\
                -H 0.1 -I 4 -l 20.0 -g -o t1.nii.gz $t1
            scil_volume_math.py convert t1_seg_2.nii.gz ${prefix}__mask_wm.nii.gz --data_type uint8
            scil_volume_math.py convert t1_seg_1.nii.gz ${prefix}__mask_gm.nii.gz --data_type uint8
            scil_volume_math.py convert t1_seg_0.nii.gz ${prefix}__mask_csf.nii.gz --data_type uint8
            mv t1_pve_2.nii.gz ${prefix}__map_wm.nii.gz
            mv t1_pve_1.nii.gz ${prefix}__map_gm.nii.gz
            mv t1_pve_0.nii.gz ${prefix}__map_csf.nii.gz

            scil_volume_stats_in_ROI.py ${prefix}__mask*.nii.gz  \
                --metrics $metrics \
                --sort_keys \
                $bin $normalize_weights > ${prefix}__stats.json

            """
        }
    ```
    </TabItem>
    </Tabs>


### Bind and Prepare the Input Structure of the Local Module in the Pipeline

#### Bind your local module

You can now include and bind your local module to the workflow, as shown in the previous steps::

```nextflow
// Add this line on the top of the main.nf
 include { STATS_METRICSINROI } from './modules/local/stats/metricsinrois/main'

 // Add the local module in the workflow
 STATS_METRICSINROI( input_channel )

 ```

### Prepare the input channel for your local module

The STATS_METRICSINROIS module requires two inputs: a T1 image and metrics. 

   <Tabs>
    <TabItem label="Task">

    You can use the `join()` and `map()` operators 
    to create an input channel that combines the T1 BET image output from the 
    `PREPROC_T1` subworkflow with the FA map from the DTI module.

    Add the input channel to the module.

    </TabItem>
    <TabItem label="Expected input channel">

        ```nextflow
        input_extract_metric = PREPROC_T1.out.image_bet
                    .join(RECONST_DTIMETRICS.out.fa)
                    .map{ it }

        STATS_METRICSINROI( input_extract_metric )
        ```
    </TabItem>
    </Tabs>


### Configure your local module

To configure your local module, you need to add each task.ext.* to the module. 

   <Tabs>
    <TabItem label="Task">

    As we’ve seen previously, use the `withName` process selector to link the `ext` 
    parameter defined in your local module to `nextflow.config` file.

    </TabItem>
    <TabItem label="Expected nextflow.config">

        ```nextflow
            withName: "STATS_METRICSINROI" {
                ext.bin = true
                ext.normalize_weights = false
            }
        ```
    </TabItem>
    </Tabs>

You now have a working local module in your Nextflow pipeline! 

    <Tabs>
    <TabItem label="Check if it works">

    ```bash
    nextflow run main.nf --input data -profile docker
    ```

    </TabItem>
    <TabItem label="Expected output">
    ```bash
    N E X T F L O W   ~  version 24.10.4

    Launching `main.nf` [dreamy_lichterman] DSL2 - revision: 1af7463284

    executor >  local (5)
    [4a/908262] DENOISING_MPPCA (sub-003_ses-01)              [100%] 1 of 1 ✔
    [29/a93bfe] RECONST_DTIMETRICS (sub-003_ses-01)           [100%] 1 of 1 ✔
    [f5/79c402] PREPROC_T1:DENOISING_NLMEANS (sub-003_ses-01) [100%] 1 of 1 ✔
    [ef/10c1d5] PREPROC_T1:BETCROP_SYNTHBET (sub-003_ses-01)  [100%] 1 of 1 ✔
    [3f/56e48b] STATS_METRICSINROI (sub-003_ses-01)           [100%] 1 of 1 ✔
    Completed at: Date Hour
    Duration    : 6m 14s
    CPU hours   : 0.1
    Succeeded   : 5
    ```

    </TabItem>
    <TabItem label="Expected main.nf">
    ```nextflow
        #!/usr/bin/env nextflow

        include { RECONST_DTIMETRICS } from './modules/nf-neuro/reconst/dtimetrics/main'
        include { DENOISING_MPPCA } from './modules/nf-neuro/denoising/mppca/main'
        include { PREPROC_T1 } from './subworkflows/nf-neuro/preproc_t1/main'
        include { STATS_METRICSINROI } from './modules/local/stats/metricsinrois/main'

        workflow get_data {
            main:
                if ( !params.input ) {
                    log.info "You must provide an input directory containing all images using:"
                    log.info ""
                    log.info "    --input=/path/to/[input]   Input directory containing your subjects"
                    log.info "                        |"
                    log.info "                        ├-- S1"
                    log.info "                        |    ├-- *dwi.nii.gz"
                    log.info "                        |    ├-- *dwi.bval"
                    log.info "                        |    ├-- *dwi.bvec"
                    log.info "                        |    └-- *t1.nii.gz"
                    log.info "                        └-- S2"
                    log.info "                             ├-- *dwi.nii.gz"
                    log.info "                             ├-- *bval"
                    log.info "                             ├-- *bvec"
                    log.info "                             └-- *t1.nii.gz"
                    log.info ""
                    error "Please resubmit your command with the previous file structure."
                }

                input = file(params.input)
                // ** Loading DWI files. ** //
                dwi_channel = Channel.fromFilePairs("$input/**/**/dwi/*dwi.{nii.gz,bval,bvec}", size: 3, flat: true)
                    { it.parent.parent.parent.name + "_" + it.parent.parent.name} // Set the subject filename as subjectID + '_' + session.
                    .map{ sid, bvals, bvecs, dwi -> [ [id: sid], dwi, bvals, bvecs ] } // Reordering the inputs.
                // ** Loading T1 file. ** //
                t1_channel = Channel.fromFilePairs("$input/**/**/anat/*T1w.nii.gz", size: 1, flat: true)
                    { it.parent.parent.parent.name + "_" + it.parent.parent.name } // Set the subject filename as subjectID + '_' + session.
                    .map{ sid, t1 -> [ [id: sid], t1 ] }
            emit:
                dwi = dwi_channel 
                anat = t1_channel
        }

        workflow {
            inputs = get_data()

            // Use Multimap to split the tuple into multi inputs structure
            ch_dwi_bvalbvec = inputs.dwi
                .multiMap { meta, dwi, bval, bvec ->
                    dwi:            [ meta, dwi ]
                    bvs_files:      [ meta, bval, bvec ]
                    dwi_bval_bvec:  [ meta, dwi, bval, bvec ]
                }

            // Denoising DWI
            input_dwi_denoise = ch_dwi_bvalbvec.dwi
                            .map{ it + [[]] }
            DENOISING_MPPCA( input_dwi_denoise )

            // Fetch specific output
            ch_dwi_denoised = DENOISING_MPPCA.out.image

            // Input DTI update with DWI denoised output
            input_dti_denoised = ch_dwi_denoised
                    .join(ch_dwi_bvalbvec.bvs_files)
                    .map{ it + [[]] }

            // DTI-derived metrics
            RECONST_DTIMETRICS( input_dti_denoised )

            // Preprocessing T1 images
            //inputs.anat.view()

            PREPROC_T1(
                inputs.anat,
                Channel.empty(),
                Channel.empty(),
                Channel.empty(),
                Channel.empty(),
                Channel.empty(),
                Channel.empty()
            )

            // Extract FA value 
            input_extract_metric = PREPROC_T1.out.image_bet
                    .join(RECONST_DTIMETRICS.out.fa)
                    .map{ it }
            //input_extract_metric.view()
            
            STATS_METRICSINROI( input_extract_metric )

        }
    ```

    </TabItem>
    <TabItem label="Expected nextflow.config">
    ```nextflow
        profiles {
            docker {
                docker.enabled          = true
                conda.enabled           = false
                singularity.enabled     = false
                podman.enabled          = false
                shifter.enabled         = false
                charliecloud.enabled    = false
                apptainer.enabled       = false
                docker.runOptions       = '-u $(id -u):$(id -g)'
            }
        }

        manifest {
            name            = 'scilus/nf-neuro-tutorial'
            description     = """nf-neuro-tutorial is a Nextflow pipeline for processing neuroimaging data."""
            version         = '0.1dev'
        }

        params.input      = false
        params.output     = 'result'

        // ** Subworkflow PREPROC T1 **
        params.preproc_t1_run_denoising = true
        params.preproc_t1_run_N4 = false
        params.preproc_t1_run_resampling = false
        params.preproc_t1_run_ants_bet = false
        params.preproc_t1_run_synthbet = true
        params.preproc_t1_run_crop = false


        process {

            publishDir = { "${params.output}/$meta.id/${task.process.replaceAll(':', '-')}" }

            withName: "DENOISING_MPPCA" {
                ext.extent = 3
            }

            withName: "BETCROP_SYNTHBET" {
                memory = "4G"
                ext.nocsf = false
            }

            withName: "RECONST_DTIMETRICS" {
                ext.ad = false
                ext.evecs = false
                ext.evals = false
                ext.fa = true
                ext.ga = false
                ext.rgb = false
                ext.md = true
                ext.mode = false
                ext.norm = false
                ext.rd = false
                ext.tensor = false
                ext.nonphysical = false
                ext.pulsation = false
                ext.residual = false
                ext.b0_thr_extract_b0 = 10
                ext.dwi_shell_tolerance = 50
                ext.max_dti_shell_value = 1200
                ext.run_qc = false
            }

            withName: "STATS_METRICSINROI" {
                ext.bin = true
                ext.normalize_weights = false
            }
        }
    ```
    </TabItem>
    </Tabs>


### Step 7 : Reorganize modules in subworkflow

In this step, we will create a local subworkflow called `PREPROC_DIFF` which include 
the two first modules used in your pipeline: DENOISING_MPPCA and RECONST_DTIMETRICS.

#### Creating a Local subworkflow structure

    Create the following local directory structure, including a main.nf file inside the subworkflow folder:

    ```
    nf-neuro-tutorial/
    ├── ...
    ├── subworkflows/
    │   ├── local/
    │   │   ├── preproc_diff/main.nf
    │   ├── nf-neuro/
    |   |   |--load_test_data
    |   |   └──preproc_t1
    └── ...
    ```

#### Write the subworkflow module

(faire le lien avec contribute ?, je dirais non, a voir)

A Nextflow subworkflow is a file containing at least two modules. 
The structure of a subworkflow should look like this:

    <Tabs>
    <TabItem label="Example of module">

    This workflow demonstrates the use of Nextflow DSL2 features such as module inclusion, 
    workflow definition with input and output channels, conditional process execution, 
    and channel manipulation.

    ```
    include { MODULE1 } from '../../../path/module1/main'
    include { MODULE2 } from '../../../path/module2/main'

    workflow SUBWORKFLOW_NAME {

        take:
            input_channel           // channel: [ val(meta), input1, input2, input3 ]

        main:

            reorganize_input_channel = ()

            // ** description MODULE 1 ** //
            if (params.run_mdoule1) {
                input_channel_module1 = ()

                MODULE1 ( input_channel_module1 )

                // Output channel
                output_option = MODULE1.out.output_name
                    .join(reorganize_input_channel.outname_2)
            }

            // ** description MODULE 2 ** //
            input_channel_module2 = ()
            MODULE2( input_channel_module2 )

        emit:
            output_module1_1     = MODULE1.out.output1     // channel: [ val(meta), file ]
            output_module1_2     = output_option           // channel: [ val(meta), file1, file2 ]
            output_module2_1     = MODULE2.out.output1     // channel: [ val(meta), file ]
            output_module2_2     = MODULE2.out.output2     // channel: [ val(meta), file ]
    }
    ```

    </TabItem>
    <TabItem label="Let's break it down">
    <Steps>
    1. `include {}`

        A subworkflow is a workflow, so it is necessary to import the modules that need to be used in the workflow.

    2. `take` : Input channels

        The workflow takes one or more input channels containing metadata and files.

    3. `main`

        This section contains the main logic of the workflow

    4. `if (params.condition*)` : conditionnal option

        Conditional option : If the condition* parameter is set, 
        the workflow performs the optionnal module. 
        This parameter must then be added to the `nextflow.config`.

    5. input channel/modules

        Next, you need to define the input channels necessary for the modules being used.

    6. `emit` : Output Channels

        The workflow emits one or more output channels.

    </Steps>

    </TabItem>
    <TabItem label="Tasks">

    The purpose of this local subworkflow is to preprocess DWI data, 
    optionally apply denoising, and compute DTI-derived metrics. 
    
    Based on the "Example of module", create a local subworkflow that integrates the following two modules:

    **DENOISING_MPPCA** – Performs MP-PCA denoising on the dMRI data (optional).

    **RECONST_DTIMETRICS** – Computes DTI metrics : FA and MD.

    
    <Steps>
    1. Import Required Modules

        Include denoising_mppca and reconst_dtimetrics in your subworkflow.

        ```nextflow
        include { RECONST_DTIMETRICS } from '../../../modules/nf-neuro/reconst/dtimetrics/main'
        include { DENOISING_MPPCA } from '../../../modules/nf-neuro/denoising/mppca/main'
        ```
    2. Rename your worflow : `PREPROC_DIFF`

    3. Define Input Channels
        
        Specify the necessary input channels for the subworkflow.
        In this case you require the files dwi, bval and bvec.

        ```nextflow
            take:
                ch_dwi           // channel: [ val(meta), dwi, bval, bvec ]

        ```

    4. Implement `main:`
    
        <Tabs>
        <TabItem label="Tasks">

        Copy and paste the relevant denoising and DTI metrics sections into the workflow. 
        Then, modify the workflow structure to include a condition that runs denoising only if the option is enabled.

        Use the parameter name : `preproc_dwi_run_denoising`.

        </TabItem>
        <TabItem label="Expected main">

        ```nextflow

        // ** Denoise DWI ** //
        if (params.preproc_dwi_run_denoising) {
            ch_dwi_bvalbvec = ch_dwi
                .multiMap { meta, dwi, bval, bvec ->
                    dwi:    [ meta, dwi ]
                    bvs_files: [ meta, bval, bvec ]
                }

            ch_denoise_dwi = ch_dwi_bvalbvec.dwi
                .map{ it + [[]] }

            DENOISING_MPPCA ( ch_denoise_dwi )

            // Fetch specific output
            ch_dwi = DENOISING_MPPCA.out.image
                .join(ch_dwi_bvalbvec.bvs_files)
        }

        // Input DTI update with DWI denoised output
        input_dti = ch_dwi.map{ it + [[]] }

        // DTI-derived metrics
        RECONST_DTIMETRICS( input_dti )
        ```
        </TabItem>
        </Tabs>

    5. Define Output Channels
    
        Emit relevant output files, including original and processed dMRI data and DTI metrics.

        ```nextflow
            emit:
                dwi                 = ch_dwi_bvalbvec.dwi           // channel: [ val(meta), dwi-raw ]
                dwi_denoised        = DENOISING_MPPCA.out.image     // channel: [ val(meta), dwi-after-mppca ]
                bvs_files           = ch_dwi_bvalbvec.bvs_files     // channel: [ val(meta), bval, bvec ]
                fa                  = RECONST_DTIMETRICS.out.fa     // channel: [ val(meta), fa ]
                md                  = RECONST_DTIMETRICS.out.md     // channel: [ val(meta), md ]
        ```

    </Steps>

    </TabItem>
    <TabItem label="Expected subworkflow">
    ```nextflow
        include { RECONST_DTIMETRICS } from '../../../modules/nf-neuro/reconst/dtimetrics/main'
        include { DENOISING_MPPCA } from '../../../modules/nf-neuro/denoising/mppca/main'

        workflow PREPROC_DIFF {

            take:
                ch_dwi           // channel: [ val(meta), dwi, bval, bvec ]

            main:

                // ** Denoise DWI ** //
                if (params.preproc_dwi_run_denoising) {
                    ch_dwi_bvalbvec = ch_dwi
                        .multiMap { meta, dwi, bval, bvec ->
                            dwi:    [ meta, dwi ]
                            bvs_files: [ meta, bval, bvec ]
                        }

                    ch_denoise_dwi = ch_dwi_bvalbvec.dwi
                        .map{ it + [[]] }

                    DENOISING_MPPCA ( ch_denoise_dwi )

                    // Fetch specific output
                    ch_dwi = DENOISING_MPPCA.out.image
                        .join(ch_dwi_bvalbvec.bvs_files)
                }

                // Input DTI update with DWI denoised output
                input_dti = ch_dwi.map{ it + [[]] }

                // DTI-derived metrics
                RECONST_DTIMETRICS( input_dti )

            emit:
                dwi                 = ch_dwi_bvalbvec.dwi           // channel: [ val(meta), dwi-raw ]
                dwi_denoised        = DENOISING_MPPCA.out.image     // channel: [ val(meta), dwi-after-mppca ]
                bvs_files           = ch_dwi_bvalbvec.bvs_files     // channel: [ val(meta), bval, bvec ]
                fa                  = RECONST_DTIMETRICS.out.fa     // channel: [ val(meta), fa ]
                md                  = RECONST_DTIMETRICS.out.md     // channel: [ val(meta), md ]

        }
    ```
    </TabItem>
    </Tabs>


#### Bind and Prepare the Input Structure of the Local subworkflow in the Pipeline

You can now include and bind your local subworkflow to the workflow, as shown in the previous steps::

```nextflow

// Add this line on the top of the main.nf
 include { PREPROC_DIFF } from './subworkflows/local/preproc_diff/main'

 // Replace the section corresponding to the sub-worflow by calling 
 // PREPROC_DIFF() in the main.nf

 PREPROC_DIFF()

 ```

As we have defined the input of the subworkflow to take the DWI image and 
the bval and bvec files, you can directly provide the input data to the workflow.

```nextflow
 PREPROC_DIFF( inputs.dwi )
 ```

Last but not least, don't forget to adapt the input channels for the DTI metrics extraction module!

```nextflow
    input_extract_metric = PREPROC_T1.out.image_bet
            .join(PREPROC_DIFF.out.fa)
            .map{ it }
```

#### Configure your Local Subworkflow

Finally, all that's left is to add the parameters defined in the subworkflow to the `nextflow.config` file.

```nextflow

    // ** subworkflow PREPROC_DIFF **
    params.preproc_dwi_run_denoising = true
```


**Well done !**

You now have a workflow with one `nf-neuro` subworkflow and your own `local` subworkflow
in your Nextflow pipeline! **Test it out !**

    <Tabs>
    <TabItem label="Check if it works">

    ```bash
    nextflow run main.nf --input data -profile docker
    ```

    </TabItem>
    <TabItem label="Expected output">
    ```bash
        N E X T F L O W   ~  version 24.10.4

        Launching `main.nf` [mighty_bose] DSL2 - revision: 7d89ad250c

        executor >  local (5)
        [6d/15e8f5] PREPROC_DIFF:DENOISING_MPPCA (sub-003_ses-01)    [100%] 1 of 1 ✔
        [18/41c27f] PREPROC_DIFF:RECONST_DTIMETRICS (sub-003_ses-01) [100%] 1 of 1 ✔
        [de/b69a00] PREPROC_T1:DENOISING_NLMEANS (sub-003_ses-01)    [100%] 1 of 1 ✔
        [e0/adaf76] PREPROC_T1:BETCROP_SYNTHBET (sub-003_ses-01)     [100%] 1 of 1 ✔
        [8f/41d4e5] STATS_METRICSINROI (sub-003_ses-01)              [100%] 1 of 1 ✔
        Completed at: 16-Mar-2025 20:00:54
        Duration    : 2m 4s
        CPU hours   : (a few seconds)
        Succeeded   : 5
    ```

    </TabItem>
    <TabItem label="Expected main.nf">
    ```nextflow
        #!/usr/bin/env nextflow

        include { PREPROC_T1 } from './subworkflows/nf-neuro/preproc_t1/main'
        include { STATS_METRICSINROI } from './modules/local/stats/metricsinrois/main'
        include { PREPROC_DIFF } from './subworkflows/local/preproc_diff/main'


        workflow get_data {
            main:
                if ( !params.input ) {
                    log.info "You must provide an input directory containing all images using:"
                    log.info ""
                    log.info "    --input=/path/to/[input]   Input directory containing your subjects"
                    log.info "                        |"
                    log.info "                        ├-- S1"
                    log.info "                        |    ├-- *dwi.nii.gz"
                    log.info "                        |    ├-- *dwi.bval"
                    log.info "                        |    ├-- *dwi.bvec"
                    log.info "                        |    └-- *t1.nii.gz"
                    log.info "                        └-- S2"
                    log.info "                             ├-- *dwi.nii.gz"
                    log.info "                             ├-- *bval"
                    log.info "                             ├-- *bvec"
                    log.info "                             └-- *t1.nii.gz"
                    log.info ""
                    error "Please resubmit your command with the previous file structure."
                }

                input = file(params.input)
                // ** Loading DWI files. ** //
                dwi_channel = Channel.fromFilePairs("$input/**/**/dwi/*dwi.{nii.gz,bval,bvec}", size: 3, flat: true)
                    { it.parent.parent.parent.name + "_" + it.parent.parent.name} // Set the subject filename as subjectID + '_' + session.
                    .map{ sid, bvals, bvecs, dwi -> [ [id: sid], dwi, bvals, bvecs ] } // Reordering the inputs.
                // ** Loading T1 file. ** //
                t1_channel = Channel.fromFilePairs("$input/**/**/anat/*T1w.nii.gz", size: 1, flat: true)
                    { it.parent.parent.parent.name + "_" + it.parent.parent.name } // Set the subject filename as subjectID + '_' + session.
                    .map{ sid, t1 -> [ [id: sid], t1 ] }
            emit:
                dwi = dwi_channel 
                anat = t1_channel
        }

        workflow {
            inputs = get_data()

            //Processing DWI
            PREPROC_DIFF( inputs.dwi )

            // Preprocessing T1 images
            //inputs.anat.view()

            PREPROC_T1(
                inputs.anat,
                Channel.empty(),
                Channel.empty(),
                Channel.empty(),
                Channel.empty(),
                Channel.empty(),
                Channel.empty()
            )

            // Extract FA value 
            input_extract_metric = PREPROC_T1.out.image_bet
                    .join(PREPROC_DIFF.out.fa)
                    .map{ it }
            
            STATS_METRICSINROI( input_extract_metric )

        }
    ```

    </TabItem>
    <TabItem label="Expected nextflow.config">
    ```nextflow
        profiles {
            docker {
                docker.enabled          = true
                conda.enabled           = false
                singularity.enabled     = false
                podman.enabled          = false
                shifter.enabled         = false
                charliecloud.enabled    = false
                apptainer.enabled       = false
                docker.runOptions       = '-u $(id -u):$(id -g)'
            }
        }

        manifest {
            name            = 'scilus/nf-neuro-tutorial'
            description     = """nf-neuro-tutorial is a Nextflow pipeline for processing neuroimaging data."""
            version         = '0.1dev'
        }

        params.input      = false
        params.output     = 'result'

        // ** subworkflow PREPROC_DIFF **
        params.preproc_dwi_run_denoising = true

        // ** Subworkflow PREPROC T1 **
        params.preproc_t1_run_denoising = true
        params.preproc_t1_run_N4 = false
        params.preproc_t1_run_resampling = false
        params.preproc_t1_run_ants_bet = false
        params.preproc_t1_run_synthbet = true
        params.preproc_t1_run_crop = false


        process {

            publishDir = { "${params.output}/$meta.id/${task.process.replaceAll(':', '-')}" }

            withName: "DENOISING_MPPCA" {
                ext.extent = 3
            }

            withName: "BETCROP_SYNTHBET" {
                memory = "4G"
                ext.nocsf = false
            }

            withName: "RECONST_DTIMETRICS" {
                ext.ad = false
                ext.evecs = false
                ext.evals = false
                ext.fa = true
                ext.ga = false
                ext.rgb = false
                ext.md = true
                ext.mode = false
                ext.norm = false
                ext.rd = false
                ext.tensor = false
                ext.nonphysical = false
                ext.pulsation = false
                ext.residual = false
                ext.b0_thr_extract_b0 = 10
                ext.dwi_shell_tolerance = 50
                ext.max_dti_shell_value = 1200
                ext.run_qc = false
            }

            withName: "STATS_METRICSINROI" {
                ext.bin = true
                ext.normalize_weights = false
            }
        }
    ```
    </TabItem>
    </Tabs>




### Step 8 : Reorganize output in result folder ( BIDS like format - Optional )

In this step, we will guide you through the process of configuring the output 
of your Nextflow pipeline to follow a BIDS-like structure. 

#### Example for one module

We will integrate specific configurations, including the use of `publishDir`, `saveAs`, and `path` 
in the `nextflow.config` file to handle the output of the process `BETCROP_SYNTHBET`.
(This configuration **must be applied to each module** in your pipeline to ensure all outputs are properly organized.)


Here is an example of the structure output :

<Tabs>
<TabItem label="Example of BIDS-like output format">

```nextflow

    // Add those parameters after the params.output

    // Publish BIDS-like configuration
    params.lean_output = true
    params.publish_dir_mode = 'copy'

    // In process {} part

    withName: "BETCROP_SYNTHBET" {
        memory = "4G"
        ext.nocsf = false
        publishDir = [
            mode: params.publish_dir_mode,
            saveAs: {
                filename ->
                def ses = meta.session ? "_${meta.session}" : ""
                if ( filename.contains("bet_image.nii.gz") ) { "${meta.id}_${ses}_desc-t1_bet.nii.gz" }
                else if ( filename.contains("brain_mask.nii.gz") ) { "${meta.id}_${ses}_desc-t1_mask.nii.gz" }
                else if ( filename.contains("versions.yml") ) { null }
                else { params.lean_output ? null : filename }
            },
            path: { meta.session ? "${params.output}/${meta.id}/${meta.session}/anat/" : "${params.output}/${meta.id}/anat/" },
            enabled: params.lean_output ? false : true
        ]
    }
```

</TabItem>
<TabItem label="Let's break it down">

<Steps>
1. `publishDir` : Specify directory

    The `publishDir` directive in Nextflow specifies where the output files from a process 
    should be saved. To match the BIDS format, we need to use dynamic paths that follow the BIDS directory structure.

    ```nextflow
    /<directory>/<subject>/<session>/<datatype>/<file>
    ```

    Where:

    **subject** is the identifier for the participant.

    **session** is the identifier for the session (if applicable).

    **datatype** refers to the type of data (e.g., `anat`, `func`).

    **file** is the actual output file (e.g., T1-weighted MRI image, functional MRI data).

2. `mode` : Mode of publishing

    The `mode` of publishing is set by a parameter, allowing flexibility in how files are published (e.g., copy, symlink, etc.).
    Define here with `params.publish_dir_mode`.

3. `SaveAs` : Name convention.

    The `saveAs` function allows you to rename or reorganize output files as they are published. 
    This is particularly helpful when you need to ensure that your output files have BIDS-compliant filenames.

    It uses the following logic:

    `def ses = meta.session ? "_${meta.session}" : ""` : It **checks** if there's a session in the metadata and creates a session string accordingly.

    `if ( filename.contains("bet_image.nii.gz") ) { "${meta.id}_${ses}_desc-t1_bet.nii.gz" }` : For specific file types (bet_image.nii.gz and brain_mask.nii.gz), it **renames** them with a structured naming convention including metadata.

    `if ( filename.contains("versions.yml") ) { null }` : It **excludes the versions.yml** file from being published.

    For other files, it either publishes them **as-is** or **excludes** them based on the `params.lean_output` setting.

4. path: Directory structure

    This defines the directory structure for publishing outputs. 
    The subject and session might be derived from the filenames or passed as inputs.

    It includes the subject (`meta.id`), session (`meta.session`), and output (`params.output`).

    Using `meta.session ? "path_with_meta_session" : "path_without_meta_session"` if no session is provided, the path will omit the session and just use the subject ID.

5. enabled: 

    This enables or disables the publishing of outputs based on the `params.lean_output` parameter. 
    If lean_output is true, publishing is disable : `params.lean_output ? false : true`

</Steps>

This configuration allows for fine-grained control over how files are renamed, where they are stored, 
and whether they are published based on specific parameters such as lean_output.

</TabItem>
</Tabs>

Make sure to apply this structure to every process in your pipeline that produces output to ensure that all data is 
organized consistently, making it easier to integrate with other BIDS-compliant tools and workflows.Additionally, 
by defining the appropriate metadata and passing the required parameters, you can easily reorganize 
your output files and make them ready for further analysis.

#### Run with BIDS-like format nextflow.config

Copy and paste this nextflow.config or write it yourself for each module and run your nextflow pipeline again.

```bash
nextflow run --input data -profile docker -resume
```

```nextflow
    profiles {
        docker {
            docker.enabled          = true
            conda.enabled           = false
            singularity.enabled     = false
            podman.enabled          = false
            shifter.enabled         = false
            charliecloud.enabled    = false
            apptainer.enabled       = false
            docker.runOptions       = '-u $(id -u):$(id -g)'
        }
    }

    manifest {
        name            = 'scilus/nf-neuro-tutorial'
        description     = """nf-neuro-tutorial is a Nextflow pipeline for processing neuroimaging data."""
        version         = '0.1dev'
    }

    params.input      = false
    params.output     = 'result'

    // Publish BIDS-like configuration
    params.lean_output = true
    params.publish_dir_mode = 'copy'

    // ** subworkflow PREPROC_DIFF **
    params.preproc_dwi_run_denoising = true

    // ** Subworkflow PREPROC T1 **
    params.preproc_t1_run_denoising = true
    params.preproc_t1_run_N4 = false
    params.preproc_t1_run_resampling = false
    params.preproc_t1_run_ants_bet = false
    params.preproc_t1_run_synthbet = true
    params.preproc_t1_run_crop = false


    process {

        //publishDir = { "${params.output}/$meta.id/${task.process.replaceAll(':', '-')}" }

        withName: "DENOISING_MPPCA" {
            ext.extent = 3
            publishDir = [
                mode: params.publish_dir_mode,
                saveAs: {
                    filename ->
                    def ses = meta.session ? "_${meta.session}" : ""
                    if ( filename.contains("denoised.nii.gz") ) { "${meta.id}_desc-denoised_dwi.nii.gz" }
                    else if ( filename.contains("versions.yml") ) { null }
                },
                path: { meta.session ? "${params.output}/${meta.id}/${meta.session}/dwi/" : "${params.output}/${meta.id}/dwi/" }
            ]
        }

        withName: "BETCROP_SYNTHBET" {
            memory = "4G"
            ext.nocsf = false
            publishDir = [
                mode: params.publish_dir_mode,
                saveAs: {
                    filename ->
                    def ses = meta.session ? "_${meta.session}" : ""
                    if ( filename.contains("bet_image.nii.gz") ) { "${meta.id}_${ses}_desc-t1_bet.nii.gz" }
                    else if ( filename.contains("brain_mask.nii.gz") ) { "${meta.id}_${ses}_desc-t1_mask.nii.gz" }
                    else if ( filename.contains("versions.yml") ) { null }
                    else { params.lean_output ? null : filename }
                },
                path: { meta.session ? "${params.output}/${meta.id}/${meta.session}/anat/" : "${params.output}/${meta.id}/anat/" },
                enabled: params.lean_output ? false : true
            ]
        }

        withName: "RECONST_DTIMETRICS" {
            ext.ad = false
            ext.evecs = false
            ext.evals = false
            ext.fa = true
            ext.ga = false
            ext.rgb = false
            ext.md = true
            ext.mode = false
            ext.norm = false
            ext.rd = false
            ext.tensor = false
            ext.nonphysical = false
            ext.pulsation = false
            ext.residual = false
            ext.b0_thr_extract_b0 = 10
            ext.dwi_shell_tolerance = 50
            ext.max_dti_shell_value = 1200
            ext.run_qc = false
            publishDir = [
                mode: params.publish_dir_mode,
                saveAs: {
                    filename ->
                    def ses = meta.session ? "_${meta.session}" : ""
                    if ( filename.contains("md.nii.gz") ) { "${meta.id}_${ses}_desc-md.nii.gz" }
                    else if ( filename.contains("fa.nii.gz") ) { "${meta.id}_${ses}_desc-fa.nii.gz" }
                    else if ( filename.contains("versions.yml") ) { null }
                    else { params.lean_output ? null : filename }
                },
                path: { meta.session ? "${params.output}/${meta.id}/${meta.session}/dwi/" : "${params.output}/${meta.id}/dwi/" }
            ]
        }

        // Here is an example where you want to store output with two different datatypes (stats + dwi)
        withName: "STATS_METRICSINROI" {
            ext.bin = true
            ext.normalize_weights = false
            publishDir = [
            [
                mode: params.publish_dir_mode,
                saveAs: {
                    filename ->
                    def ses = meta.session ? "_${meta.session}" : ""
                    if ( filename.contains("stats.json") ) { "${meta.id}_${ses}_desc-dti_stats.json" }
                    else { params.lean_output ? null : filename }
                },
                path: { meta.session ? "${params.output}/${meta.id}/${meta.session}/stats/" : "${params.output}/${meta.id}/stats/" }
            ],
            [
                mode: params.publish_dir_mode,
                saveAs: {
                    filename ->
                    def ses = meta.session ? "_${meta.session}" : ""
                    if ( filename.contains("map_csf.nii.gz") ) { "${meta.id}_${ses}_desc-t1_map_csf.nii.gz" }
                    else if ( filename.contains("map_wm.nii.gz") ) { "${meta.id}_${ses}_desc-t1_map_wm.nii.gz" }
                    else if ( filename.contains("map_gm.nii.gz") ) { "${meta.id}_${ses}_desc-t1_map_gm.nii.gz" }
                    else if ( filename.contains("mask_csf.nii.gz") ) { "${meta.id}_${ses}_desc-t1_mask_csf.nii.gz" }
                    else if ( filename.contains("mask_wm.nii.gz") ) { "${meta.id}_${ses}_desc-t1_mask_wm.nii.gz" }
                    else if ( filename.contains("mask_gm.nii.gz") ) { "${meta.id}_${ses}_desc-t1_mask_gm.nii.gz" }
                    else { params.lean_output ? null : filename }
                },
                path: { meta.session ? "${params.output}/${meta.id}/${meta.session}/anat/" : "${params.output}/${meta.id}/anat/" }
            ]
            ]
        }
    }
```

```bash
nextflow run --input data -profile docker -resume
```

:::note
Note that once the pipeline has run, you can experiment with different `publishDir` options by using the 
resume feature and changing the `params.output` parameter to `result_2` or another directory. 
Nextflow will simply copy the images from the work directory to the new output folder, following the output definitions you have set.
:::



:::caution
Do cannot set both a `global` publishDir (`publishDir = { "${params.output}/$meta.id/${task.process.replaceAll(':', '-')}" }`)
and a publishDir for each individual module at the same time. 
(Is it useful ???) 
:::


### Dataset 

You can find example **input data** [here](https://openneuro.org/datasets/ds004513/versions/1.0.4). Take
note that the dataset is very large, consider using a **data management system** like datalab
to download a single subject. You are also free to use any of your own data, as long
as they respect the directory structure defined [above](#create-a-prototype-pipeline).
:::
