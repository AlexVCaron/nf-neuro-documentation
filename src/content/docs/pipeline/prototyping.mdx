---
title: Tutoriel to create your pipeline using components from nf-neuro
description: Create a simple pipeline using nf-neuro components.
---

import CommandOutputs from '../../../components/CommandOutputs.astro';
import { Steps } from '@astrojs/starlight/components';
import { Tabs, TabItem } from '@astrojs/starlight/components';

## Welcome to the nf-neuro tutorial!

In this tutorial, you'll learn how to setup your pipeline environment and create your
main pipeline workflow, using components from `nf-neuro`. 

This tutorial is aimed at anyone wishing to use nf-neuro 
and/or develop their own pipeline. The tutorial is designed to be used as 
a free resource; you can follow the different steps at any time. 

The material to follow the tutorial is available here: https://github.com/scilus/nf-neuro-tutorial. 
You can get it using :

```bash

git clone git@github.com:yourgithubID/nf-neuro-tutorial.git
cd nf-neuro-tutorial
```

## Environment configuration

To get setup fast, we recommend using **VS Code** and the `development container`. Follow the
[documentation](/nf-neuro/custom-pipeline/setup) to do so. You can also use
[those instructions](/nf-neuro/custom-pipeline/setup#manual-installation) to setup yourself manually.


## Tutorial overview

The aim of this tutorial is to create a simple pipeline using nf-neuro components to process a 
diffusion image and a T1 image, and to extract mean diffusivity (MD) and fractional anisotropy (FA) 
values from the white matter, grey matter and cerebrospinal fluid (CSF) maps.

The tutorial consists of 11 sequential steps, described below, which will enable you to complete 
the pre-filled main.nf and nextflow.config files to obtain a complete workflow.

The tutorial folder is pre-configured so that you can easily follow the different steps. 
The config, tests, modules and subworkflows folders contain the pre-installed nf-neuro components, 
while the data folder contains the data provided for the tutorial.

```
nf-neuro-tutorial
              |
              ├-- .devcontainer
              ├-- config
              ├-- data
              ├-- modules              
              ├-- subworkflows
              ├-- tests
              ├-- .gitignore
              ├-- .nf-core.yml
              ├-- README.md
              ├-- main.nf
              ├-- modules.json
              ├-- nextflow.config
              └-- nf-test.config
```

### `nextflow.config`

The `nextflow.config` file contains **parameters** that users can change when calling your pipeline
(prefixed with `params.`) and default configurations for execution. Here is an example of a basic
`nextflow.config` file :

```nextflow
params.input      = false
params.output     = 'result'

profiles {
    docker {
        docker.enabled          = true
        conda.enabled           = false
        singularity.enabled     = false
        podman.enabled          = false
        shifter.enabled         = false
        charliecloud.enabled    = false
        apptainer.enabled       = false
        docker.runOptions       = '-u $(id -u):$(id -g)'
    }
}

manifest {
    name            = 'scilus/nf-neuro-tutorial'
    description     = """nf-neuro-tutorial is a Nextflow pipeline for processing neuroimaging data."""
    version         = '0.1dev'
}
```

The parameters defined with `params.` can be changed at execution by another `nextflow.config` file or
by supplying them as arguments when calling the pipeline using `nextflow run` :

```bash
nextflow run main.nf --input /path/to/input --output /path/to/output
```

:::caution
If using a version of `nextflow` prior to `22.03.0-edge` (or `22.04.0` if using only stable releases),
you need to add `-dsl2` to the `nextflow run` command to enable the DSL2 syntax in which your
pipeline and `nf-neuro` are written.
:::

### `main.nf`

This file is your pipeline execution file. It contains all modules and subworkflows you want to run, and the
channels that define how data passes between them. This is also where you define how to fetch your input files.
This can be done using a workflow definition, here is an example for a basic usage:

```nextflow
#!/usr/bin/env nextflow

workflow get_data {
    main:
        if ( !params.input ) {
            log.info "You must provide an input directory containing all files using:"
            log.info ""
            log.info "    --input=/path/to/[input]   Input directory containing the file needed"
            log.info "                        |"
            log.info "                        └-- Input"
            log.info "                             └-- participants.*"
            log.info ""
            error "Please resubmit your command with the previous file structure."
        }

        input = file(params.input)
        // ** Loading all files. ** //
        participants_channel = Channel.fromFilePairs("$input/participants.*", flat: true)
            { "participants_files" }

    emit:
        participants = participants_channel
}

workflow {
    // ** Now call your input workflow to fetch your files ** //
    data = get_data()
    data.participants.view()
}
```

This workflow, named get_data, is designed to fetch input files from a specified directory.
It's a generic and simple data loading step that's often used at the beginning of a larger pipeline.

The use of channels is key here - they allow for efficient, asynchronous data flow through the pipeline.
The fromFilePairs method is particularly useful for handling paired-end sequencing data, though in this case it's being used more generally to group related files.

### Data

To keep things simple, we'll consider you want to process a dataset that contains for one subject and session a DWI and a T1 image, as follows :

```
data
    |--dataset_description.json
    |--participants.json
    |--participants.tsv
    ├-- sub-003
        |-- ses-01
            |-- anat
            |   |-- sub-003_ses-01_T1w.json
            |   |-- sub-003_ses-01_T1w.nii.gz
            |-- dwi
                |-- sub-003_ses-01_dir-AP_dwi.bval
                |-- sub-003_ses-01_dir-AP_dwi.bvec
                |-- sub-003_ses-01_dir-AP_dwi.json
                |-- sub-003_ses-01_dir-AP_dwi.nii.gz

```

## Start to play with the tutorial

### Step 1: Visualize input data (main branch)

Open the main.nf file int the VScode viewer. 
The `main.nf` is pre-filled with a worflow name `get_data` to fetch data from the data folder and a worflow 
to visualize the data. Run the nextflow pipeline using the following command : 

   <CommandOutputs>
   <span slot="command">
   ```bash
   nextflow run main.nf --input data -profile docker
   ```
   </span>
   <span slot="output">
   You should see this output :

   ```bash frame="none"      
      [participants_files, /workspaces/nf-neuro-tutorial_test/data/participants.json, 
   /workspaces/nf-neuro-tutorial_test/data/participants.tsv]
   ```
   </span>
   </CommandOutputs>


### Step 2: Create input structure for DTI module

Now, let's modify the `get_data` workflow to fetch the test data. 
Replace the provided `get_data` workflow with the one below and re-execute the nextflow.

   <CommandOutputs>
   <span slot="command">
    ```nextflow
    #!/usr/bin/env nextflow

    // include { RECONST_DTIMETRICS } from './modules/nf-neuro/reconst/dtimetrics/main'

    workflow get_data {
        main:
            if ( !params.input ) {
                log.info "You must provide an input directory containing all images using:"
                log.info ""
                log.info "    --input=/path/to/[input]   Input directory containing your subjects"
                log.info "                        |"
                log.info "                        ├-- S1"
                log.info "                        |    └-- ses-01"
                log.info "                        |    |    ├-- anat"
                log.info "                        |    |    |   |--*t1.nii.gz"
                log.info "                        |    |    |--dwi"
                log.info "                        |    |    |   |--*dwi.nii.gz"
                log.info "                        |    |    |   ├-- *dwi.bval"
                log.info "                        |    |    |   └-- *dwi.bvec"
                log.info "                        |    └-- ses-02"            
                log.info "                        └-- S2"
                log.info "                             └-- ses-01"
                log.info "                             |     ├-- anat"
                log.info "                             |    |   |--*t1.nii.gz"
                log.info "                             |    |--dwi"
                log.info "                             |    |   |--*dwi.nii.gz"
                log.info "                             |    |   ├-- *dwi.bval"
                log.info "                             |    |   └-- *dwi.bvec"
                log.info "                             └-- ses-02" 
                log.info ""
                error "Please resubmit your command with the previous file structure."
            }

            input = file(params.input)
            // ** Loading all files. ** //
            dwi_channel = Channel.fromFilePairs("$input/**/**/dwi/*dwi.{nii.gz,bval,bvec}", size: 3, flat: true)

        emit:
            dwi = dwi_channel
    }

    workflow {
        // ** Now call your input workflow to fetch your files ** //
        data = get_data()
        data.dwi.view() // Contains your DWI data: [meta, dwi, bval, bvec]
    }
    ```

    ```bash
    nextflow run main.nf --input data -profile docker
    ```

   </span>
   <span slot="output">
   You should see this output:

   ```bash frame="none"      
   [sub-003_ses-01_dir-AP, /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/dwi/sub-003_ses-01_dir-AP_dwi.bval, /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/dwi/sub-003_ses-01_dir-AP_dwi.bvec, /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/dwi/sub-003_ses-01_dir-AP_dwi.nii.gz]
   ```
    The output is a channel where each element is a tuple containing:

    A key (likely the base name of the files)
    The matching .nii.gz file (the DWI image)
    The matching .bval file
    The matching .bvec file

    ```bash frame="none"
    [subject_session_id, /path/to/subject1/ses-01/dwi/*dwi.nii.gz, /path/to/subject1/ses-01/dwi/*dwi.bval, /path/to/subject1/ses-01/dwi/*dwi.bvec]
    ```

   </span>
   <span slot="explain">
    Let's explain it step by step:

    1. "$input/**/**/dwi/*dwi.{nii.gz,bval,bvec}": This is the glob pattern used to match files. 
    It's looking for files in any subdirectory of $input parameter that are in a dwi folder and have 
    names ending with dwi.nii.gz, dwi.bval, or dwi.bvec.

    2. size: 3: This option specifies that each emitted item should contain 3 files 
    (in this case, the dwi.nii.gz, dwi.bval, and dwi.bvec files).

    3. flat: true: This option flattens the output, so instead of emitting a list of files,
    it emits the files as separate elements in the tuple.
   
   </span>
   </CommandOutputs>


Now let's modify the input structure to make `sub-003_ses-01_dir-AP` become `sub-003_ses-01` 
using the following structure and `it` :  

   <CommandOutputs>
   <span slot="command">
    ```nextflow
            input = file(params.input)
            // ** Loading all files. ** //
            dwi_channel = Channel.fromFilePairs("$input/**/**/dwi/*dwi.{nii.gz,bval,bvec}", size: 3, flat: true)
                { it.parent.parent.parent.name + "_" + it.parent.parent.name}
    ```

   </span>
   <span slot="output">
   You should see this output:

   ```bash frame="none"      
    [sub-003_ses-01, /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/dwi/sub-003_ses-01_dir-AP_dwi.bval, /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/dwi/sub-003_ses-01_dir-AP_dwi.bvec, /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/dwi/sub-003_ses-01_dir-AP_dwi.nii.gz]
   ```
    The output is a channel where each element is a tuple containing:

    A key (likely the base name of the files)
    The matching .nii.gz file (the DWI image)
    The matching .bval file
    The matching .bvec file


   </span>
   <span slot="explain">
    Let's explain it step by step:

    { it.parent.parent.parent.name + "_" + it.parent.parent.name}
    
    This is a closure that defines how to create the grouping key for the file pairs. 
    It's using the names of the parent directories to create a unique identifier, 
    so you need to add as many "parent" as necessary to fit your data structure.

    To get subjectID `sub-003` : 
    it
      .parent > `sub-003`
        .parent > ses-01
          .parent > dwi
            .name > sub-003_ses-01_dir-AP_dwi.bval

    To get sessionNumber `ses-01`
    it
      .parent > `ses-01`
        .parent > dwi
          .name > sub-003_ses-01_dir-AP_dwi.bval
   
   </span>
   </CommandOutputs>



Finally, by default, files are sorted alphabetically, so you need to reorder them to get a specific file order. 
To do this, you use the `map` function, as follows:


   <CommandOutputs>
   <span slot="command">
    ```nextflow
           input = file(params.input)
           // ** Loading all files. ** //
           dwi_channel = Channel.fromFilePairs("$input/**/**/dwi/*dwi.{nii.gz,bval,bvec}", size: 3, flat: true)
               { it.parent.parent.parent.name + "_" + it.parent.parent.name}
               .map{ sid, bvals, bvecs, dwi -> [ [id: sid], dwi, bvals, bvecs ] } // Reordering the inputs.
    ```

   </span>
   <span slot="output">
   You should see this output:

   ```bash frame="none"      
    [[id:sub-003_ses-01], /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/dwi/sub-003_ses-01_dir-AP_dwi.nii.gz, /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/dwi/sub-003_ses-01_dir-AP_dwi.bval, /workspaces/nf-neuro-tutorial_test/data/sub-003/ses-01/dwi/sub-003_ses-01_dir-AP_dwi.bvec]
   ```

   </span>
   <span slot="explain">
    Let's explain it:

    ```bash frame="none" 
    .map{ sid, bvals, bvecs, dwi -> [ [id: sid], dwi, bvals, bvecs ] }
    ```
    This is a map operation that reorganizes the emitted data. 
    It takes the sid (sample ID), bvals, bvecs, and dwi files and reorganizes them into a new structure.

    The output is a channel where each element is a tuple containing:

       [id: sid] corresponds to [id:sub-003_ses-01]
       dwi corresponds to the dwi.nii.gz file
       bvals corresponds to the .bval file
       bvecs corresponds to the .bvec file

   </span>
   </CommandOutputs>


You have now created a file structure that is designed to be easily used in subsequent pipeline processes, 
with the subject ID and session clearly labeled and the files required for DWI processing 
(the image file, b-values, and b-vectors).



## Install new module (stop here)

Now, you can install the modules you want to include in your pipeline. Let's import the `denoising/nlmeans` module
for T1 denoising. To do so, first open a terminal using the **VS Code** interface, either using the main menu
`Terminal > New Terminal` or the shortcut ``ctrl+shit+` ``. Then, use the `nf-core modules install` command.

```bash
nf-core modules install denoising/nlmeans
```

:::note
On a first run of the commands, you may get prompted to configure some aspects of `nf-core`. You can accept every
prompt you see.
:::

:::caution
If you get an error saying `nf-core` command doesn't exists, then `poetry` has failed to load in the terminal
correctly. First, close your terminal, open a new one and try again. If the tool still cannot be found, try the
command `poetry shell`, then running `nf-core modules install` again. If this does not solve the problem, [open an
issue](https://github.com/scilus/nf-neuro/issues/new?template=bug_report.md) on the `nf-neuro` repository.
:::

To use it in your pipeline, you need to import it at the top of your `main.nf` file. You can do it using the
`include { DENOISING_NLMEANS } from ./modules/nf-neuro/denoising/nlmeans/main.nf` statement. Once done, you can
use `DENOISING_NLMEANS` in your pipeline and feed your inputs to it! To have a look at which files are required to
run the module, use the `nf-core modules info denoising/nlmeans` command (if you are using **VS Code**, install the
`nextflow` extension, that gives you hints on modules and subworkflows intputs).

## Complete `main.nf` example

A complete example (e.g., fetching the inputs, importing the module, and supplying the inputs to the modules)
is given below:

```nextflow
#!/usr/bin/env nextflow

include { DENOISING_NLMEANS } from './modules/nf-neuro/denoising/nlmeans/main.nf'

workflow get_data {
    main:
        if ( !params.input ) {
            log.info "You must provide an input directory containing all images using:"
            log.info ""
            log.info "    --input=/path/to/[input]   Input directory containing your subjects"
            log.info "                        |"
            log.info "                        ├-- S1"
            log.info "                        |    ├-- *dwi.nii.gz"
            log.info "                        |    ├-- *dwi.bval"
            log.info "                        |    ├-- *dwi.bvec"
            log.info "                        |    ├-- *revb0.nii.gz"
            log.info "                        |    └-- *t1.nii.gz"
            log.info "                        └-- S2"
            log.info "                             ├-- *dwi.nii.gz"
            log.info "                             ├-- *bval"
            log.info "                             ├-- *bvec"
            log.info "                             ├-- *revb0.nii.gz"
            log.info "                             └-- *t1.nii.gz"
            log.info ""
            error "Please resubmit your command with the previous file structure."
        }

        input = file(params.input)
        // ** Loading all files. ** //
        dwi_channel = Channel.fromFilePairs("$input/**/*dwi.{nii.gz,bval,bvec}", size: 3, flat: true)
            { it.parent.name }
            .map{ sid, bvals, bvecs, dwi -> [ [id: sid], dwi, bvals, bvecs ] } // Reordering the inputs.
        rev_channel = Channel.fromFilePairs("$input/**/*revb0.nii.gz", size: 1, flat: true)
            { it.parent.name }
            .map{ sid, rev -> [ [id: sid], rev ] }
        anat_channel = Channel.fromFilePairs("$input/**/*t1.nii.gz", size: 1, flat: true)
            { it.parent.name }
            .map{ sid, t1 -> [ [id: sid], t1 ] }
    emit: // Those three lines below define your named output, use those labels to select which file you want.
        dwi = dwi_channel
        rev = rev_channel
        anat = anat_channel
}

workflow {
    inputs = get_data()
    // ** Create the input channel for nlmeans. ** //
    // **  - Note that it also can take a mask as input, but it is not required. ** //
    // **  - Replacing it by an empty list here. ** //
    ch_denoising = inputs.anat
        .map{ it + [[]] } // This add one empty list to the channel, since we do not have a mask.

    // ** Run DENOISING_NLMEANS ** //
    DENOISING_NLMEANS( ch_denoising )
    DENOISING_NLMEANS.out.image.view() // This will show the output of the module.

    // ** You can then reuse the outputs and supply them to another module/subworkflow! ** //
    //ch_nextmodule = DENOISING_NLMEANS.out.image
    //  .join(ch_another_file)
    // NEXT_MODULE( ch_nextmodule )
}
```

### Fetching outputs from the modules

You now have a working `main.nf` file. You could execute the pipeline, but the outputs would be hard to access. Let's define the
`publishDir` into which to place them using the `nextflow.config` file and the `output` parameter we defined earlier :

```nextflow
process {
    publishDir = { "${params.output}/$meta.id/${task.process.replaceAll(':', '-')}" }
}
```

:::caution
Here, `meta` is a special variable, defined in every **module**, a map that gets passed around with the data, into which you can
put information. Beware however, as it is also used to **join channels together** by looking at there whole content.
:::

### Defining modules parameters

Once this is done, you might want to supply parameters for some of your modules that could be modified when calling the pipeline.
To know which parameters are accepted in your modules, refer to the `main.nf` of the specific `nf-neuro` module and look for parameters
that are prefixed with `ext.`, placed just before its **bash script**. `denoising/nlmeans` takes 1 possible parameter, `number_of_coils`,
that we add to the `nextflow.config` file :

```nextflow
params.number_of_coils = 1
```

The last step is to bind your parameters to the specific module they are meant for. This is done using a **process selector** (`withName`), that
links the `ext.` parameter to the `params.` parameter :

```nextflow
process {
    withName: 'DENOISING_NLMEANS' {
        ext.number_of_coils = params.number_of_coils
    }
}
```

That's it! Your `nextflow.config` should look something like this:

```
params.input      = false
params.output     = 'output'

params.number_of_coils = 1

docker.enabled    = true
docker.runOptions = '-u $(id -u):$(id -g)'

process {
    publishDir = { "${params.output}/$meta.id/${task.process.replaceAll(':', '-')}" }

    withName: 'DENOISING_NLMEANS' {
        ext.number_of_coils = params.number_of_coils
    }
}
```

:::note
Once your pipeline is built, or when you want to test it, run :
```
nextflow run main.nf --input <directory>
```

You can find example **input data** [here](https://openneuro.org/datasets/ds004513/versions/1.0.4). Take
note that the dataset is very large, consider using a **data management system** like datalab
to download a single subject. You are also free to use any of your own data, as long
as they respect the directory structure defined [above](#create-a-prototype-pipeline).
:::
